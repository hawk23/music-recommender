% v2-acmsmall-sample.tex, dated March 6 2012
% This is a sample file for ACM small trim journals
%
% Compilation using 'acmsmall.cls' - version 1.3 (March 2012), Aptara Inc.
% (c) 2010 Association for Computing Machinery (ACM)
%
% Questions/Suggestions/Feedback should be addressed to => "acmtexsupport@aptaracorp.com".
% Users can also go through the FAQs available on the journal's submission webpage.
%
% Steps to compile: latex, bibtex, latex latex
%
% For tracking purposes => this is v1.3 - March 2012

\documentclass[prodmode,acmtecs]{acmsmall} % Aptara syntax

% Package to generate and customize Algorithm as per ACM style
\usepackage[ruled]{algorithm2e}

% Own packages
\usepackage[utf8]{inputenc} % for German quotation marks
\usepackage[ngerman]{babel} % for German Umlaute
\usepackage{listings} % for source code
\usepackage{amsmath} % eqref
\lstset{numbers=left, numberstyle=\tiny, numbersep=5pt} \lstset{language=Python}

\renewcommand{\algorithmcfname}{ALGORITHM}
\SetAlFnt{\small}
\SetAlCapFnt{\small}
\SetAlCapNameFnt{\small}
\SetAlCapHSkip{0pt}
\IncMargin{-\parindent}

% Metadata Information
\acmVolume{0}
\acmNumber{0}
\acmArticle{1}
\acmYear{2015}
\acmMonth{10}

% Copyright
%\setcopyright{acmcopyright}
%\setcopyright{acmlicensed}
\setcopyright{rightsretained}
%\setcopyright{usgov}
%\setcopyright{usgovmixed}
%\setcopyright{cagov}
%\setcopyright{cagovmixed}

% DOI
%\doi{0000001.0000001}

%ISSN
%\issn{1234-56789}

% Default fixed font does not support bold face
\DeclareFixedFont{\ttb}{T1}{txtt}{bx}{n}{9} % for bold
\DeclareFixedFont{\ttm}{T1}{txtt}{m}{n}{9}  % for normal

% Custom colors
\usepackage{color}
\definecolor{deepblue}{rgb}{0,0,0.5}
\definecolor{deepred}{rgb}{0.6,0,0}
\definecolor{deepgreen}{rgb}{0,0.5,0}

% Python style for highlighting
\newcommand\pythonstyle{\lstset{
language=Python,
basicstyle=\ttm,
otherkeywords={self},             % Add keywords here
keywordstyle=\ttb\color{deepblue},
emph={MyClass,__init__},          % Custom highlighting
emphstyle=\ttb\color{deepred},    % Custom highlighting style
stringstyle=\color{deepgreen},
frame=tb,                         % Any extra options here
showstringspaces=false            % 
}}


% Python environment
\lstnewenvironment{python}[1][]
{
\pythonstyle
\lstset{#1}
}
{}

% Python for external files
\newcommand\pythonexternal[2][]{{
\pythonstyle
\lstinputlisting[#1]{#2}}}

% Python for inline
\newcommand\pythoninline[1]{{\pythonstyle\lstinline!#1!}}

% Document starts
\begin{document}

% Page heads
\markboth{}{Music Similarity and Retrieval: Content- and Context-based Approaches}

% Title portion
\title{Report on practical exercises for \emph{Music Similarity and Retrieval: Content- and Context-based Approaches}}
\author{
Verena Dittmer
\affil{Alpe Adria University of Klagenfurt}
Mario Graf
\affil{Alpe Adria University of Klagenfurt}
Peter Luca Lidl
\affil{Alpe Adria University of Klagenfurt}
}

%
% The code below should be generated by the tool at
% http://dl.acm.org/ccs.cfm
% Please copy and paste the code instead of the example below. 
%
\begin{CCSXML}
<ccs2012>
<concept>
<concept_id>10002951.10003317.10003347.10003352</concept_id>
<concept_desc>Information systems~Information extraction</concept_desc>
<concept_significance>300</concept_significance>
</concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[300]{Information systems~Information extraction}

%
% End generated code
%

% We no longer use \terms command
%\terms{Design, Algorithms, Performance}

% \keywords{TODO, TODO, TODO}

\begin{bottomstuff}
This is a lot of bottom stuff.
\end{bottomstuff}

\maketitle


\section{Introduction}
Um die Arbeit in der Gruppe zu erleichtern haben wir zwei Projekte auf Github angelegt. Der gesamte Source-Code ist dort einsehbar:
\begin{itemize}
\item \textbf{music-similarity-miner: } https://github.com/hawk23/music-similarity-miner/
\item \textbf{music-recommender: } https://github.com/hawk23/music-recommender/
\end{itemize}

\section{Similarity Miner}
Die Implementierung des Similarity Miners ist in dem Repository music-similarity-miner \cite{music-similarity-miner} zu finden. Wir haben uns beschlossen eine eigene Implementierung für den Similarity Miner zu schreiben und nicht die zur Verfügung gestellte Vorlage zu verwenden. Der von uns implementierte Crawler (\lstinline$music-similarity-miner\crawler$) nutzt die Suchmaschine DuckDuckGo um 80 Suchergebnisse zu jedem Artist herunterzuladen. Als Suchbegriff haben wir \lstinline$"[Artist-Name] music"$ verwendet um genauere bzw. passenderer Suchergebnisse zu erhalten. Als Output liefert der Crawler für jeden Artist ein Textfile, das die bereits heruntergeladenen Websites enthält.

Das Preprocessing der HTML-Dokumente, das Term Weighting der Terme und die Bestimmung der Ähnlichkeit von zwei Künstlern wird in der /tf\_idf/main.py ausgeführt.
Hier kann man drei Skriptparameter übergeben: 
\begin{python}
main.py [data-Ordner]['standard_tf_idf' oder 'alternative_tf_idf']
['jaccard_measure' oder 'cosine_measure']
\end{python}
Wenn keine Parameter übergeben werden, wird standardmäßig \texttt{'../data standard\_tf\_idf jaccard\_measure'} als Argument gesetzt. 


\subsection{Preprocessing}
Für das Preprocessing wurde der bereitgestellte Code verwendet. Dieses verarbeitet die HTML-Dokumente um zu einem Bag of Words für jeden Künstler zu gelangen. Dieser wurde um die Performance zu verbessern angepasst, sodass das Preprocessing mit Threads durchgeführt wird. Jeder \texttt{Prepocessing}-Thread erhält dabei den Dateipfad zu einem Künstler als Konstruktor-Argument. Das Ergebnis des Preprocessings wird in ein Dictionary mit dem Künstler als Key und den durch das Preprocessing erhaltenen Wörtern als Value gespeichert. Aufgrunddessen dass mehrere Threads auf das Dictionary zugreifen, wird dieses beim Schreiben des Ergebnisses durch ein \texttt{RLock} gelockt.

Für unseren Term Index verwenden wir das Online-Musikwörterbuch von Artopium. Dieses wurde von \url{http://musicterms.artopium.com/} heruntergeladen.
Aufgrund der Verwendung eines Musikwörterbuchs müssen auch die Index Terme das Preprocessing durchlaufen, damit gleiche Terme korrekt erkannt werden. Andernfalls würden beispielsweise \glqq durchfuhrung\grqq (das Ergebnis des Preprocessing auf das Künstlerdokument) mit \glqq Durchfuhrung\grqq (aus dem Musikwörterbuch) verglichen werden.

\subsection{Term Weighting}
Um die Wichtigkeit von jedem Term für einen bestimmten Künstler zu bestimmen, wird das Term Weighting verwendet. Dabei wird mit der Gewichtungsfunktion Term Frequency-Inverse Document Frequency (TF-IDF) die Gewichtung bestimmt. Dafür werden für die Standard-TF-IDF zuerst die folgenden Parameter benötigt: $N$ als die Anzahl der Dokumente, $f_{d,t}$ als die Anzahl der Vorkomnisse eines Terms $t$ in einem Dokument $d$ und $f_t$ als die Anzahl der Dokumente, in dem der Term $t$ vorkommt. 

$N$ ist bei uns die Anzahl der Künstler, bzw. die Anzahl der HTML-Dokumente, da für jeden Künstler ein Dokument erstellt wurde. $f_{d,t}$ und $f_t$ müssen bestimmt werden. Hierfür wird die Klasse \texttt{TermCounter} verwendet. Diese zählt für jedes Dokument bzw. Künstler die Anzahl der  Vorkomnisse jedes Terms aus dem Term Index. Dies wird in dem Dictionary \texttt{artists\_with\_terms\_count} gespeichert. Die Künstler als Keys haben jeweils ein Dictionary als Values. Dieses besteht wiederum aus den Terms aus den Term Index als Keys und den Anzahl der Vorkomnissen für diesen Künstler als Values. 
Somit kann man beispielsweise folgendermaßen auf den Vorkomnisse des Terms \glqq four\grqq vom Künstler \glqq Alicia Keys\grqq zugreifen:
\begin{python}
    print artists_with_terms_count['Alicia Keys']['four']
\end{python}
Aufgrund der Verwendung eines Dictionaries wird gesichert, dass die Termreihenfolge eindeutig bestimmt ist, da eine Hashfunktion verwendet wird um einen Term auf die Position zu mappen. Die Reihenfolge der Terme ist zwar nicht lexikalisch geordnet, aber für jeden Künstler gleich. Dies kann man auch beobachten, wenn man sich die bei der Ausführung erstellte Datei /data/similarities/counts anschaut.

Zur Bestimmung von $f_t$ verwendet die statische Methode \texttt{count\_documents\_containing\_term(term)} in der Klasse \texttt{TermCounter} nun dieses Dictionary. Sie überprüft für jeden Artist, ob der Wert für den Term größer 0 ist, falls ja, so wird der Counter \texttt{count} um eins erhöht.
\begin{python}
    @staticmethod
    def count_documents_containing_term(term):
        count = 0
        for artist in artists_with_terms_count.keys():
            if term in artists_with_terms_count[artist] and \ 		       artists_with_terms_count[artist][term] > 0:
                count += 1
        return count
\end{python}       

Nun können wir für alle Dokumente jedem Term im Term Index eine Gewichtung zuweisen. Dafür können verschiedene TF-IDF Funktionen verwendet werden. In unserer Implementierung wird die TF\_C mit der IDF\_B2 (die zumeist in der Praxis verwendet wird) standardmäßig verwendet. Als Alternative zu dieser Funktion wurde die TF\_C2 mit der IDF\_E implementiert. 

Die TF Formulierung bezieht sich auf die zweite Monotonitäts Annahme: Die Wichtigkeit eines Terms für ein bestimmtes Dokument ist höher, wenn der Term öfters vorkommt.
Die Formel TF\_C \eqref{eqTFC} und TF\_C2 \eqref{eq:TFC2} lauten wie folgt:
\begin{equation}\label{eqTFC}
  r_{d,t} = 1 + log_{e} f_{d,t}
\end{equation}
\begin{equation}\label{eq:TFC2}
  r_{d,t} = log_{e} (1 + f_{d,t})
\end{equation}

Dabei sind die beiden Formeln sehr ähnlich: TF\_C hat jedoch den Nachteil, dass die Logarithmusfunktion für $f_dt$ gleich Null nicht definiert ist. In diesem Fall wird in der Implementierung für die Gewichtung dieses Terms Null zurückgegeben (da der Term somit auch nicht in dem Dokument vorkommt und keine Wichtigkeit hat). 

Die IDF behandelt die erste Monotonitäts Annahme: Seltene Terme sind nicht weniger wichtig als Häufige, was insofern bedeutet dass die Wichtigkeit eines Terms für ein Dokument höher ist, wenn der Term in nur wenigen Dokumenten vorkommt.
\begin{equation}\label{eq:IDFB2}
  w_{t} = log_{e} \frac{N}{f_{t}}
\end{equation}
\begin{equation}\label{eq:IDFC}
  w_{t} = \frac{1}{f_{t}}
\end{equation}
Beide Formeln beachten diese Monotonitäts Annahme: Ist bei IDF\_B2 \eqref{eq:IDFB2} $f_t$ kleiner, so wird die Gewichtung des IDF-Wertes logarithmisch größer. Ebenso gilt auch für IDF\_C \eqref{eq:IDFC}, dass wenn $f_t$ kleiner wird, so wird der IDF-Wert invers proportional größer und der Term somit wichtiger. Die beiden Funktionen unterscheiden sich prinzipiell in dem Logarithmus von IDF\_B2: Dies begründet sich auf Zipfs Gesetz, das besagt dass die Häufigkeit eines Wortes innerhalb eines Dokuments invers proportional zu dem Platz, in dem das Wort in der nach den Häufigkeiten sortierten Frequenztabelle ist. Die Verwendung des Logarithmus führt dazu, dass die großen Vorkomnisse von ganz wenigen Wörtern stark reduziert wird.

Die TF Formel wird mit der IDF Formel multipliziert und man erhält die Gewichtung. In der Klasse \texttt{WeightMeasurer} ist die Implementierung vorzufinden. Mittels dem zweiten optionalen Parameter der Datei /tf\_idf/main.py kann man auswählen, welche der beiden TF-IDF Funktionen angewendet werden soll (\glqq standard\_tf\_id\grqq oder \glqq alternative\_tf\_id\grqq).

\subsection{Similarity Computation}
Um die Ähnlichkeit von zwei Künstlern zu bestimmen wird eine Ähnlichkeitsfunktion verwendet. Die bekannteste hierbei ist die Cosine Similarity (Kosinus-Ähnlichkeit) \eqref{eq:cosine}. Diese ermittelt den Abstand zweier Vektoren, wobei eine Normierung passiert, da die Länge der Dokumente berücksichtigt wird. 
\begin{equation}\label{eq:cosine}
  S_{d_1, d_2} = \frac{\sum_{t \in T_{d_1, d_2}}{w_{d_1, t} \cdot w_{d_2, t}}}{W_{d_1} \cdot W_{d_2}}
\end{equation}

Die zweite Ähnlichkeitsfunktion, die bei unserer Implementierung der Ähnlichkeitsbestimmung standardmäßig verwendet wird, ist die Jaccard-Ähnlichkeit \eqref{eqjaccard}. 
\begin{equation}\label{eqjaccard}
  S_{d_1, d_2} = \frac{\sum_{t \in T_{d_1, d_2}}{w_{d_1, t} \cdot w_{d_2, t}}}{W_{d_1}^2 + W_{d_2}^2 - \sum_{t \in T_{d_1, d_2}}{w_{d_1, t} \cdot w_{d_2, t}}}
\end{equation}
Die Jaccard-Ähnlichkeit wird aus der Durchschnittsmenge der beiden Vektoren (die Terme, die bei beiden Dokumenten vorkommen) dividiert durch die Vereinigung der beiden Vektoren gebildet.

Die Implementierung der Jaccard- und der Kosinusfunktion sind in der Klasse \texttt{SimilarityMeasurer} zu finden. Auch hier kann man mittels der Parameterübergabe (als dritter optionaler Parameter) an die /tf\_idf/main.py bestimmen, welche der beiden Funktionen ausgeführt wird.

\section{Music Recommendation}
In der Aufgabenstellung B wurde verlangt ein Musik Empfehlungssystem zu implementieren und zu evaluieren \cite{music-recommender}. Nachdem bereits ein großer Teil des Musik Empfehlungssystems vorgegeben wurde, haben wir uns auf die offenen Punkte fokussiert. Damit das Empfehlungssystem funktioniert, werden als erstes Eingabedaten gebraucht. Die bereitgestellte Datei seed\_users.txt stellt bereits einige Benutzernamen zur Verfügung, jedoch enthält diese nicht die empfohlene Benutzeranzahl von zumindest fünf hundert. Daher haben wir uns im ersten Schritt damit beschäftigt diese Mindestanforderungen zu erfüllen. Nachdem die Aufgabenstellung das Verwenden der LastFM API verlangte und keiner zuvor damit gearbeitet hat, haben wir uns einen Account angelegt und versucht einen API Schlüssel zu beantragen. Leider scheiterten wir bereits in diesem Schritt, da durch den derzeitigen Umbau der API einige Links nicht mehr verfügbar waren. Relativ bald haben wir jedoch einen alten API Schlüssel von einem anderen Programmierer im Internet gefunden und haben uns vorübergehend erlaubt diesen zu verwenden. Während dem Einarbeiten in die von der LastFM API bereitgestellten Funktionalität, haben wir bereits einige passende Funktionen für unsere Aufgabenstellung gefunden. Unter diesen Funktionen waren beispielsweise die folgenden:

\begin{itemize}
\item \textbf{Chart.GetTopArtists()} haben wir verwendet um eine eindeutige Liste von den in den Charts derzeit führenden Künstlern abzurufen.
\begin{python}
def lastfm_api_call_getTopArtists():
    content_merged = []        # empty list

    # Construct API call
    url = LASTFM_API_URL + "?method=chart.gettopartists" + \
          "&format=" + LASTFM_OUTPUT_FORMAT + \
          "&api_key=" + LASTFM_API_KEY + \
          "&limit=" + str(MAX_ARTISTS)

    content = urllib.urlopen(url).read()

    # Add retrieved content of current page to merged content variable
    content_merged.append(content)
    json_content = json.loads(content)

    artist_list = []

    for _artist in range(0, MAX_ARTISTS):
        artist_list.append((json_content["artists"]["artist"][_artist]["name"]).encode("utf-8"))

    return artist_list
\end{python}  

\item \textbf{Artist.GetTopFans()} haben wir verwendet um für einen angegeben Künstler eine eindeutige Liste von Fans (also Benutzern) abzurufen.
\begin{python}
def lastfm_api_call_getTopFans(artist_list):
    content_merged = []        # empty list
    user_list = ""

    # Construct API call
    for _artist in range(0, MAX_ARTISTS):
        url = LASTFM_API_URL + "?method=artist.gettopfans" + \
          "&api_key=" + LASTFM_API_KEY + \
          "&artist=" + artist_list[_artist] + \
          "&format=" + LASTFM_OUTPUT_FORMAT

        _content = urllib.urlopen(url).read()

        # Add retrieved content of current page to merged content variable
        content_merged.append(_content)
        json_content = json.loads(_content)

        for _user in range(0, MAX_FANS):
            user_list += (json_content["topfans"]["user"][_user]["name"]).encode("utf-8") + '\n'

    # Write content to local file
    output_file = "./users.txt"
    file_out = open(output_file, 'w')
    file_out.write(artist_lis
\end{python}

\item \textbf{User.GetFriends()} haben wir verwendet um für einen angegeben Benutzer eine eindeutige Liste von Freunden (also Benutzern) abzurufen.
\begin{python}
def lastfm_api_call_getFriends(user):
    content_merged = []         # empty list
    friend_list = []            # empty list

    # Construct API call
    url = LASTFM_API_URL + "?method=user.getfriends" + \
        "&api_key=" + LASTFM_API_KEY + \
        "&user=" + str(user) + \
        "&format=" + LASTFM_OUTPUT_FORMAT

    _content = urllib.urlopen(url).read()

    # Add retrieved content of current page to merged content variable
    content_merged.append(_content)
    json_content = json.loads(_content)

    if "friends" in json_content.keys():
        for _friend in json_content["friends"]["user"]:
            friend_list.append(_friend["name"].encode("utf-8"))

    return friend_list
\end{python}
\end{itemize}

Nach unseren ersten Testläufen mussten wir jedoch feststellen, dass gewisse von uns verwendete Methode keine validen Ergebnisse zurückliefern. Nach längerer Quellcodeanalyse und Recherche im Internet fanden wir heraus, dass der Fehler nicht in unserer Implementierung, sondern am derzeitigen Umbau der LastFM API lag. Gewisse verwendete Funktionen wurden nämlich noch nicht vollständig funktionsfähig portiert. Nach längerem Probieren, welche Funktionen nun wirklich verfügbar waren, hatten wir schlussendlich eine Datenbasis von ca. 1200 Usern erstellt.

Bevor wir den Listening Event Fetcher erneut gestartet haben, wollten wir die Verteilung der Benutzer, bezüglich Herkunft, Alter sowie den Fakt ob diese häufig aktiv sind, berücksichtigen. Hiervon haben wir uns eine Verbesserung unseres Empfehlungssystems erhofft. Leider stießen wir auch hier auf viele Probleme mit Funktionen des LastFM Frameworks. Diese Funktionen sollten wichtige Informationen über Benutzer liefern. Ferner wurden viele Benutzer durch diese Klassifizierung unbrauchbar, da keine oder falsche Informationen vorlagen, wodurch wir entschieden diese Klassifizierung wieder zu entfernen.

\textbf{Update}:
Nachdem die Abgabefrist auf eine Woche verlängert wurde und der LastFm\_LE\_Fetcher nicht optimale Ergebnisse lieferte wurden nun von Herrn Schedl neue Testdaten zur Verfügung gestellt. Damit diese Datensätze richtig geparst werden können, mussten wir in der Converter\_LE-UAM den abgefragten Index ändern, da nun keine Tracks mehr vorhanden sind.

\textbf{Evaluate\_Recommender.py}: In diesem Abschnitt wird die Berechnung von F1, Baseline und k-nearest beschrieben.
Der nachfolgende Quellcode beschreibt wie wir die F1 Berechnung, unter Verwendung von der durchschnittlichen Precision und dem durchschnittlichen Recall, durchgeführt haben.

\begin{python}
	# calculate f1 measure
	f1 = 2 * ((avg_prec * avg_rec) / (avg_prec + avg_rec))
\end{python}

Um die Qualität der Recommendations beurteilen zu können, haben wir eine Baseline-Methode implementiert. Die Baseline-Methode wählt für den User eine zufällige Anzahl an Artists, aus für die noch keine Listening-Events (für den betreffenden User) existieren.

\begin{python}
# This function defines a baseline recommender, which selects a random 
# number of artists the seed user hasn't listened yet and returns these. 
# Since this function is used with a cross fold validation all artists 
# not in the seed_aidx_train set are artists the user hasn't listened to.
def recommend_baseline (UAM, seed_uidx, seed_aidx_train):
    # UAM               user-artist-matrix
    # seed_uidx         user index of seed user

    # Get list of artist indices the user hasn't listened yet
    all_artists_idx = range(0, len(UAM[0,:]))
    not_listened = np.setdiff1d(all_artists_idx, seed_aidx_train)

    # get number of artists to recommend
    num_recommend = randint(1,len(not_listened))

    # recommend artists
    recommended_artists_idx = [not_listened[randint(0,len(not_listened)-1)] for _ in range(num_recommend)]

    # return result with possible duplicates removed
    return list(set(recommended_artists_idx))
\end{python}

Der Collaborative-Filtering-Recommender wurde außerdem von uns derart erweitert, dass er parametrisierbar mehr als nur den nähesten Nachbarn berücksichtigt. Dazu haben wir die CF-Recommendations auf Basis der k nähesten Nachbarn vereinigt und als Empfehlung für den User zurückgeliefert.

\begin{python}
def recommend_CF(UAM, seed_uidx, seed_aidx_train, K = 1):
    ...

    # Select the k closest neighbor to seed user (which is the last but one; last one is user u herself!)
    kneighbor_idx = sort_idx[-(1+K):-1]

    artist_idx_n = [] # indices of artists user u's neighbor(s) listened to
    for neighbor_idx in kneighbor_idx:
        # indices of artists user u's neighbor listened to
        listened_to = np.nonzero(UAM[neighbor_idx, :])
        # np.nonzero returns a tuple of arrays, so we need to take the first element only
        artist_idx_n = np.union1d(listened_to[0], artist_idx_n)
    ...
\end{python}

\subsection{Evaluierung}
Wir haben mehrere Testläufe durchgeführt. Einen mit der von uns neu implementierten Baseline Funktion sowie der von uns auf k-nearest erweiterten CF Funktion mit unterschiedlicher Parametriesierung. Die nachfolgende Auflistung zeigt unsere Ergebnisse für die genannten Testläufe:

\begin{center}
	\resizebox{!}{1.5cm} {
    \begin{tabular}{ | l | l | l | l |}
    \hline
    \multicolumn{4}{|c|}{rec\_aidx = recommend\_CF(copy\_UAM, u, train\_aidx, K)} \\ \hline
    MAP & MAR & F1 & K \\ \hline
    0.16 & 0.81 & 0.27 & K = 1 \\ \hline
    0.14 & 1.39 & 0.26 & K = 2 mit Vereinigung \\ \hline
    0.13 & 3.07 & 0.25 & K = 4 mit Vereinigung \\ \hline
    0.13 & 5.08 & 0.26 & K = 8 mit Vereinigung \\ \hline
    0.13 & 36.52 & 0.26 & Baseline \\ 
    \hline
    \end{tabular}
    }
\end{center}

% Start of "Sample References" section

% \section{Typical references in new ACM Reference Format}
% A paginated journal article \cite{Abril07}, an enumerated
% journal article \cite{Cohen07}, a reference to an entire issue \cite{JCohen96}.

% Appendix
% \appendix
% \section*{APPENDIX}
% \setcounter{section}{1}

% Bibliography
\bibliographystyle{ACM-Reference-Format-Journals}
\bibliography{msrcc-report-bibfile}
                             % Sample .bib file with references that match those in
                             % the 'Specifications Document (V1.5)' as well containing
                             % 'legacy' bibs and bibs with 'alternate codings'.
                             % Gerry Murray - March 2012


\medskip

\end{document}
% End of v2-acmsmall-sample.tex (March 2012) - Gerry Murray, ACM
